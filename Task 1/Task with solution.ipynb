{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2c397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9f30f",
   "metadata": {},
   "source": [
    "### Task 1: Regular expressions\n",
    "In this task you are asked to create regular expressions that meet the specified conditions. Please use the ``re`` package for the following subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83245d4",
   "metadata": {},
   "source": [
    "< < The tasks has been removed as they are too simple to shpwcase onto github > >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e4d62",
   "metadata": {},
   "source": [
    "### Task 2: Finding the most similar word\n",
    "The goal of this task is, given a corpus, to find the most similar word for a provided word. As an example we will consider the King James Bible that is is included in the ``gutenberg`` corpus and we are looking to find the word that is most similar to ``god``. We consider two words similar if they appear in the same word context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036fc75",
   "metadata": {},
   "source": [
    "__a) Cleaning the input__\n",
    "\n",
    "Write a function that, given a list of tokens, returns a list of tokens that we consider valid for our task. Moreover, all input tokens should be converted to lower case.\n",
    "\n",
    "An *invalid* token is a token that \n",
    "- is a punctuation mark (consider `string.punctuation`).\n",
    "- is entirely numeric digits (e.g. `\"123\"`)\n",
    "- optionally, if `remove_stopwords=True` then stopwords in the English language are also not considered valid tokens (use nltk's stopwords). \n",
    "\n",
    "Use the function signature specified in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975b8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "import string\n",
    "\n",
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :param remove_stopwords: bool indicating if stopwords should be removed\n",
    "                             False by default\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    valids=[]\n",
    "    for token in tokens:\n",
    "      token=token.lower()\n",
    "      if (remove_stopwords==True): #check if stopwords should be removed (True)\n",
    "        if(token not in stop): #check if the token is a stop word\n",
    "          if(not token.isdigit()): #check if the token is numeric\n",
    "            if(token not in string.punctuation): #check if the token is a punctuation mark\n",
    "              valids.append(token)\n",
    "      else: #stopwords should not be removed so we repeat just the 2 other checks\n",
    "        if(not token.isdigit()):\n",
    "            if(token not in string.punctuation):\n",
    "              valids.append(token)\n",
    "    return valids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64541015",
   "metadata": {},
   "source": [
    "good. rather than nesting if statements could `continue` to skip current token when condition is meat. better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac98e2",
   "metadata": {},
   "source": [
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    valid = []\n",
    "    punct = string.punctuation\n",
    "    stop = stopwords.words('english')\n",
    "    digit = re.compile(r\"\\d+\")\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in punct:\n",
    "            continue\n",
    "        if remove_stopwords and t.lower() in stop:\n",
    "            continue\n",
    "        if re.fullmatch(digit, t):\n",
    "            continue\n",
    "        valid.append(t.lower())\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81184587",
   "metadata": {},
   "source": [
    "__b) Counting the surroundings__\n",
    "\n",
    "In our simple model of word similarity we consider words as similar if they are being used in the same context (i.e. they have similar words surrounding them). \n",
    "\n",
    "Implement a function that, given a list of words, returns the count of all words in a designated word's surrounding. The context size indicates how many words to the left and right we consider, i.e. a context size of 1 indicates that we only consider the words immediately before and after a central word to be in its context. \n",
    "\n",
    "Your function should return a dictionary which maps each word $w$ from the input list to a dictionary. The inner dictionary should map each word that appears in the context of the central word $w$ to a number that indicates how often it appears in the context of $w$.\n",
    "\n",
    "Make sure your implementation has linear complexity in the vocabulary / corpus length. Use the function signature specified in the cell below.\n",
    "\n",
    "__Hint:__ Instead of the inner dictionary you can alternatively use `Counter` or `defaultdict` which can be found in the Python module `collections`. Moreover, consider the function ``ngrams`` from ``nltk``.\n",
    "\n",
    "__Example:__ _For the input_\n",
    "`['hi', 'james', 'how', 'are', 'you', 'hello', 'catherine', 'how', 'are', 'you']` _and_ `context_size=1`\n",
    "_we wish to obtain:_\n",
    "```\n",
    "{'hi': {'james': 1},\n",
    " 'james': {'hi': 1, 'how': 1},\n",
    " 'how': {'james': 1, 'are': 2, 'catherine': 1},\n",
    " 'are': {'how': 2, 'you': 2},\n",
    " 'you': {'are': 2, 'hello': 1},\n",
    " 'hello': {'you': 1, 'catherine': 1},\n",
    " 'catherine': {'hello': 1, 'how': 1}}\n",
    "```\n",
    "__Explanation:__ _The word_ `hi` _is only surrounded by_ `james`_. The word_ `james` _is surrounded by_ `hi` _and_ `how` _. The word_ `how` _is surrounded by_ `james`_, by_ `catherine` _and by_ `are` _twice, ..._\n",
    "\n",
    "_For_ `contextsize=2` _we obtain:_\n",
    "```\n",
    "{'hi': {'james': 1, 'how': 1},\n",
    "'james': {'hi': 1, 'how': 1, 'are': 1},\n",
    "'how': {'are': 2, 'you': 2, 'hi': 1, 'james': 1, 'hello': 1, 'catherine': 1},\n",
    "'are': {'how': 2, 'you': 2, 'james': 1, 'hello': 1, 'catherine': 1},\n",
    "'you': {'how': 2, 'are': 2, 'hello': 1, 'catherine': 1},\n",
    "'hello': {'are': 1, 'you': 1, 'catherine': 1, 'how': 1},\n",
    "'catherine': {'you': 1, 'hello': 1, 'how': 1, 'are': 1}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a911633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding_counts(tokens: Union[List[str], StreamBackedCorpusView], context_size: int) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens\n",
    "    :param context_size: integer that indicates the number of context words that are considered on both sides of the central word\n",
    "    :return: dict of dicts that holds the count of context words for each input token\n",
    "    \"\"\"\n",
    "    full = {}\n",
    "    for i in set(tokens):\n",
    "        temp={}\n",
    "#         print(i)\n",
    "        for k in [j for j,x in enumerate(tokens) if x==i]:\n",
    "            for count in range(1,context_size+1):\n",
    "                ind = count + k\n",
    "                if ind < len(tokens) :    \n",
    "                    if tokens[ind] in temp:\n",
    "                        temp[tokens[ind]]+=1\n",
    "                    else:\n",
    "                        temp[tokens[ind]]=1\n",
    "                ind = k - count\n",
    "                if ind >=0:\n",
    "                    if tokens[ind] in temp:\n",
    "                        temp[tokens[ind]]+=1\n",
    "                    else:\n",
    "                        temp[tokens[ind]]=1\n",
    "        full[i]=temp\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57bde",
   "metadata": {},
   "source": [
    "seems to give correct result but too inefficient. consider using collections.counter and looping over nltk ngrams\n",
    "\n",
    "from nltk.util import ngrams\n",
    "n_grams = ngrams(tokens, 2*context_size+1, pad_left=True, pad_right=True) # padding: count context words at start/end of list\n",
    "\n",
    "for gram in n_grams:\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e226d",
   "metadata": {},
   "source": [
    "__c) Keeping the top $k$ words in context__\n",
    "\n",
    "To reduce the size of our result from task __b)__, we will only consider the most frequent context words for each token. Therefore, write a function that keeps only the $k$ most frequent words in the context of a designated word. Ties are resolved in lexicographic order (e.g. _**A**braham_ would be preferred to _**B**ethlehem_). The function should return a dictionary that maps each word in the outer dictionary to a __set__ of their top $k$ most frequent context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106f0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sets(context_dict: Dict[str, Dict[str, int]], k: int) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    :param context_dict: dict of dicts that holds the count of context words for each word\n",
    "    :param k: integer that specifies how many context words should be kept\n",
    "    :return: dict that maps each word to a set of its k most frequent context words\n",
    "    \"\"\"\n",
    "    sorted_ct={}\n",
    "    for key,v in context_dict.items():\n",
    "        sorted_ct[key] = set(dict(sorted(v.items(), key=lambda item: (-item[1],item[0]))[:int(k)]).keys())\n",
    "    return sorted_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84aa14",
   "metadata": {},
   "source": [
    "__d) Finding the most similar words__\n",
    "\n",
    "Given a dictionary as obtained in task __c)__ and a word $w$, we want to find the words that have the highest similarity to $w$ in terms of their context. We operationalize context similarity with the Jaccard index (https://en.wikipedia.org/wiki/Jaccard_index).\n",
    "The Jaccard index of two sets $A$ and $B$ (in our case the sets of context words) is defined as:\n",
    "\n",
    "$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "Write a function that returns the words that have the highest similarity to an input word $w$ (ignoring the input word itself). Since several words may have the same Jaccard similarity to the input word, your function should return the set of words that are most similar to $w$ and the respective value of the Jaccard similarity. Use the function signature specified in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_words(input_word: str, contexts: Dict[str, Set[str]]) -> tuple[Set[str], float]:\n",
    "    \"\"\"\n",
    "    :param input_word: string that represents the word we are interested in\n",
    "    :param contexts: dict that maps each word to a set of its most frequent context words\n",
    "    :returns:\n",
    "        - set of the most similar words to the input word\n",
    "        - float that indicates the highest Jaccard similarity to the input word\n",
    "    \"\"\"\n",
    "    input_set = contexts.pop(input_word, None)\n",
    "    dis_set={}\n",
    "    if input_set:\n",
    "        input_list = list(input_set)\n",
    "        for word,word_set in contexts.items():\n",
    "            word_list= list(word_set)\n",
    "            union = list(set(word_list) | set(input_list))\n",
    "            intersection = list(set(word_list) & set(input_list))\n",
    "            distance = len(intersection)/len(union)\n",
    "            if distance in dis_set:\n",
    "                dis_set[distance].add(word)\n",
    "            else:\n",
    "                dis_set[distance]={word}\n",
    "        sorted_set = dict(sorted(dis_set.items(), key=lambda item: -item[0]))\n",
    "        jac_dis = list(sorted_set.keys())[0]\n",
    "        return (sorted_set[jac_dis],jac_dis)\n",
    "    else:\n",
    "        print(\"Word not present\")\n",
    "        return (0,0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0214f",
   "metadata": {},
   "source": [
    "__e) Bringing it all together__\n",
    "\n",
    "Finally, we want to apply our functions to the King James Bible (`'bible-kjv.txt'`) that is part of the ``gutenberg`` corpus. We intend to find the word(s) that is (are) most similar to ``god``. Follow the steps below:\n",
    "\n",
    "i) Obtain a list of all tokens from the King James Bible and store it in a variable ``tokens``.  \n",
    "\n",
    "ii) Clean the list of tokens with your function from __a)__ to get the list of valid tokens (without removing stopwords) and store it in a variable ``valid_tokens``.  \n",
    "\n",
    "iii) Apply your function from __b)__ to count the context words for all valid tokens with a ``context_size`` of 2 and store the result in a variable ``context_counts``.  \n",
    "\n",
    "iv) Using your function from __c)__, keep only the 20 most frequent words in a valid tokens context and store the result in a variable ``context_words``.  \n",
    "\n",
    "v) Finally, find the most similar words to the word _god_ with your function from __d)__ and store the set of most similar words in the variable ``set_god`` and the highest Jaccard similarity in the variable ``sim_god``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec089bae-1c82-48b1-8e2c-b3660f46a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/parzival/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cfb84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010654\n",
      "792050\n"
     ]
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "print(len(words))\n",
    "valid_tokens = get_valid_tokens(words,False)\n",
    "print(len(valid_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766381b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following cell takes about 5 mins to run\n",
    "\n",
    "context_counts = get_surrounding_counts(valid_tokens,2)\n",
    "# print(context_counts)\n",
    "context_words = to_sets(context_counts,20)\n",
    "# print(context_words)\n",
    "set_god,sim_god = find_most_similar_words(\"god\",context_words)\n",
    "print(set_god,sim_god)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
