{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ef36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple, Sequence\n",
    "from numpy.typing import NDArray\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2e19a",
   "metadata": {},
   "source": [
    "### Task 1: Markov chains\n",
    "\n",
    "In this task we want to create a language model by using the independence assumption af Markov. We therefore assume that the probability of a word is only dependent on a fixed number of preceding words. E.g. by restricting the number of preceding words to $n$ we can approximate the probability of a word $w_{i}$ following a sequence of words $w_1, ..., w_{i-1}$ by:\n",
    "\n",
    "$P(w_{i}|w_1, ..., w_{i-1}) \\approx P(w_{i}|w_{i-n}, ..., w_{i-1})$\n",
    "\n",
    "We will first train our model on a given corpus and then use it to automatically generate text.\n",
    "\n",
    "Throughout this task we will define a single class with different functions. If you're unsure how to access class methods and attributes, take a look at the documentation (https://docs.python.org/3/tutorial/classes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bade6",
   "metadata": {},
   "source": [
    "__a) Collecting the counts__\n",
    "\n",
    "Write a function `process_corpus` that takes a corpus of text (as a sequence of tokens) as input and counts how often an n-gram of length $n$ (``context_len=n``) is followed by a certain word (the n-grams should __not__ be padded). The function should return a dictionary that maps every n-gram that is observed in the corpus to an inner dictionary. The inner dictionary maps each word to a number, that indicates how often the word succeeds the n-gram in the given corpus. We will need these counts to compute the conditional probabilities $P(w_{i}|w_{i-n}, ..., w_{i-1})$.\n",
    "Additionally, also return the entire vocabulary $V$ (i.e. the set of all unique tokens that appear in the corpus).\n",
    "\n",
    "Make sure your implementation is efficient by using e.g. ``Counter`` and ``defaultdict`` from the package ``collections``.   \n",
    "\n",
    "__b) Conditional probabilities__\n",
    "\n",
    "Write a function `transition_prob` that takes an n-gram $(w_{i-n}, ..., w_{i-1})$ and a word $w_{i}$ of the vocabulary $V$ as input and returns the conditional probability that the given n-gram is followed by the input word $w_{i}$. Recall that this conditional probability can be computed as follows:\n",
    "\n",
    "$P(w_{i}|w_{i-n}, ..., w_{i-1}) = \\frac{\\text{Count}(w_{i-n}, ..., w_{i-1}, w_{i})}{\\sum_{w \\in V}\\text{Count}(w_{i-n}, ..., w_{i-1}, w)}$\n",
    "\n",
    "If the n-gram has never been observed, return $\\frac{1}{|V|}$.\n",
    "\n",
    "__c) Most likely word__\n",
    "\n",
    "Write a function `most_likely_word` that gets an n-gram as input and returns the word that is most likely to succeed the given n-gram. In case there are multiple words that are equally likely to follow the given n-gram, return all of them. \n",
    "Note that you should **not** loop over the **entire** vocabulary to obtain the most likely word.\n",
    "In case the given n-gram has never been observed, return the entire vocabulary.\n",
    "\n",
    "__d) Generating text__\n",
    "\n",
    "Write a function `generate_text` that generates a text sequence of length `k`, given a starting sequence of words (`ngram`). The function should choose the most likely next word in every step; in case there are multiple equally likely words, randomly choose one. You should return a list of ``k`` words, that includes the starting sequence and is the most probable continuation. \n",
    "\n",
    "\n",
    "Please do not implement other functions for the MarkovModel class.\n",
    "\n",
    "Use the function signatures in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1fb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from typing import Sequence, Dict, Set, Tuple, List, Union\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "import string\n",
    "import random\n",
    "\n",
    "class MarkovModel():\n",
    "    '''Markov model for generating text.'''\n",
    "\n",
    "    def __init__(self, tokens: Sequence[str], context_len: int):\n",
    "        '''\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :param context_len: length of the n-gram (number of preceding words)\n",
    "        '''\n",
    "        self.context_len = context_len\n",
    "        self.counts, self.v = self.process_corpus(tokens)\n",
    "\n",
    "    def process_corpus(self, tokens: Sequence[str]) -> tuple[Dict[Tuple[str, ...], Dict[str, int]], Set]:\n",
    "        '''Training method of the model, counts the occurrences of each word after each observed n-gram.\n",
    "        :param tokens: text corpus on which the model is trained on as an iterator of tokens\n",
    "        :returns:\n",
    "            - nested dict that maps each n-gram to the counts of the words succeeding it\n",
    "            - the whole vocabulary as a set\n",
    "        '''\n",
    "        ngram_counts = defaultdict(Counter)\n",
    "        valids = []\n",
    "\n",
    "        # Directly perform token cleaning operations\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if not token.isdigit() and token not in string.punctuation:\n",
    "                valids.append(token)\n",
    "\n",
    "        vocabulary = set(valids)\n",
    "        n_grams = list(ngrams(valids, self.context_len + 1))\n",
    "\n",
    "        for ngram in n_grams:\n",
    "            prefix = ngram[:-1]\n",
    "            next_word = ngram[-1]\n",
    "            ngram_counts[prefix][next_word] += 1\n",
    "\n",
    "        ngram_dict = {ngram: dict(counter) for ngram, counter in ngram_counts.items()}\n",
    "        return ngram_dict, vocabulary\n",
    "\n",
    "    def transition_prob(self, ngram: Tuple[str, ...], word: str) -> float:\n",
    "        '''Compute the conditional probability that the input word follows the given n-gram.\n",
    "        :param ngram: string tuple that represents an n-gram\n",
    "        :param word: input word\n",
    "        :return: probability that the n-gram is followed by the input word\n",
    "        '''\n",
    "        if ngram not in self.counts:\n",
    "            print(\"ngram not in vocabulary\")\n",
    "            return 1 / len(self.v)\n",
    "\n",
    "        next_word_counts = self.counts[ngram]\n",
    "        total_count = sum(next_word_counts.values())\n",
    "        word_count = next_word_counts.get(word, 0)\n",
    "        return word_count / total_count\n",
    "\n",
    "    def most_likely_word(self, ngram: Tuple[str, ...]) -> Set[str]:\n",
    "        '''Computes which word is most likely to follow a given n-gram.\n",
    "        :param ngram: n-gram we are interested in\n",
    "        return: set of words that are most likely to follow the n-gram\n",
    "        '''\n",
    "        if ngram not in self.counts:\n",
    "            return self.v\n",
    "\n",
    "        next_word_counts = self.counts[ngram]\n",
    "        max_count = max(next_word_counts.values())\n",
    "        most_likely_words = [word for word, count in next_word_counts.items() if count == max_count]\n",
    "        return set(most_likely_words)\n",
    "\n",
    "    def generate_text(self, ngram: Tuple[str, ...], k: int) -> List[str]:\n",
    "        '''Generates a text sequence of length k, given a starting sequence.\n",
    "        :param ngram: starting sequence\n",
    "        :param k: total number of words in the generated sequence\n",
    "        :return: sequence of generated words, including the starting sequence\n",
    "        '''\n",
    "        generated_text = list(ngram)\n",
    "\n",
    "        current_ngram = ngram\n",
    "\n",
    "        for _ in range(k - len(ngram)):\n",
    "            most_likely_words = self.most_likely_word(current_ngram)\n",
    "            next_word = random.choice(list(most_likely_words))\n",
    "            generated_text.append(next_word)\n",
    "\n",
    "            # Update the current n-gram\n",
    "            current_ngram = tuple(generated_text[-self.context_len:])\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06341054",
   "metadata": {},
   "source": [
    "__e) Apply the model to a corpus__\n",
    "\n",
    "Finally, we want to apply our functions to the King James Bible (`'bible-kjv.txt'`) that is part of the ``gutenberg`` corpus. Use the function from HA 1, task 2a) to obtain a list of valid tokens (do not remove stopwords) from the King Jame Bible.\n",
    "\n",
    "Initialize the MarkovModel with the list of valid tokens and ``context_len=3`` and answer the following subtasks:\n",
    "\n",
    "i) What is the probability that the word ``babylon`` follows the sequence ``the king of``? \n",
    "\n",
    "ii) What are the most likely words to follow the sequence ``the world is``? \n",
    "\n",
    "iii) Generate a sequence of length 20 that starts with ``mother mary was``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "902941ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability , 0.1906779661016949\n",
      "Most Likely word ,  {'sown', 'gone', 'very', 'nigh', 'not'}\n",
      "Generated words ,  ['mother', 'mary', 'was', 'espoused', 'to', 'joseph', 'before', 'they', 'came', 'together', 'she', 'was', 'found', 'with', 'child', 'of', 'gilead', 'that', 'behold', 'the']\n"
     ]
    }
   ],
   "source": [
    "x= MarkovModel(gutenberg.words('bible-kjv.txt'),3)\n",
    "trans_prob = x.transition_prob(('the','king','of'),'babylon')\n",
    "print(\"Transition Probability ,\",trans_prob)\n",
    "most_likely_word = x.most_likely_word(('the','word','is'))\n",
    "print(\"Most Likely word , \",most_likely_word)\n",
    "generated_text = x.generate_text(('mother','mary','was'),20)\n",
    "print(\"Generated words , \",generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc6b92",
   "metadata": {},
   "source": [
    "### Task 2: POS tagging\n",
    "\n",
    "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTK’s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01d59e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/parzival/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/parzival/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /home/parzival/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK's off-the-shelf POS tagger\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from typing import List, Dict, Set\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from nltk import pos_tag\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78bb0d",
   "metadata": {},
   "source": [
    "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5651149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "\n",
    "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
    "    '''\n",
    "    :param corpus: sequence of tokens that represents the text corpus\n",
    "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
    "    '''\n",
    "\n",
    "    tagged_corpus = pos_tag(corpus)  \n",
    "    tag_to_words = {}\n",
    "    \n",
    "    for word, tag in tagged_corpus:\n",
    "        if tag not in tag_to_words:\n",
    "            tag_to_words[tag] = set()\n",
    "        tag_to_words[tag].add(word)\n",
    "    \n",
    "    return tag_to_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737321",
   "metadata": {},
   "source": [
    "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag. \n",
    "\n",
    "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context. \n",
    "\n",
    "You can assume that the training corpus is large enough to include all possible POS tags. \n",
    "\n",
    "_Hint: consider the_ ``random`` _module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b4efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool = False) -> List[List[str]]:\n",
    "    '''\n",
    "    :param sentence: the input sentence as a list of tokens\n",
    "    :param pos_dict: dictionary that maps each tag to a set of tokens\n",
    "    :param n: number of sequences to generate\n",
    "    :param better_quality: if True, ensures the tag structure of output sentences matches the input\n",
    "    :return: list of generated sentences\n",
    "    '''\n",
    "    sentences = []\n",
    "    tagged_sent = pos_tag(sentence)\n",
    "\n",
    "    for _ in range(n):\n",
    "        new_sentence = []\n",
    "        for word, tag in tagged_sent:\n",
    "            if tag in pos_dict:\n",
    "                new_word = random.choice(list(pos_dict[tag]))\n",
    "                new_sentence.append(new_word)\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        \n",
    "        if better_quality:\n",
    "            new_tagged_sent = pos_tag(new_sentence)\n",
    "            for i, (new_word, new_tag) in enumerate(new_tagged_sent):\n",
    "                original_tag = tagged_sent[i][1]\n",
    "                if new_tag != original_tag:\n",
    "                    if original_tag in pos_dict:\n",
    "                        new_sentence[i] = random.choice(list(pos_dict[original_tag]))\n",
    "\n",
    "        sentences.append(new_sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b3ba",
   "metadata": {},
   "source": [
    "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
    "\n",
    "* \"Emma\" by Jane Austen\n",
    "\n",
    "* The \"King James Bible\"\n",
    "\n",
    "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a69ab118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da9ac341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Those', 'delay', 'engages', 'badly', 'silver'],\n",
       "  ['this', 'protestation', 'improves', 'apprehensively', 'open-hearted'],\n",
       "  ['envy', 'wound', 'says', 'lovely', 'well-grown'],\n",
       "  ['these', 'phrase', 'hopes', 'stoutly', 'insidious'],\n",
       "  ['No', 'ballroom', 'hopes', 'just', 'mournful'],\n",
       "  ['Both', 'subject.', 'tells', 'moreover', 'nourishing'],\n",
       "  ['The', 'ashamed', 'Does', 'naturally', 'parish'],\n",
       "  ['Some', 'analogy', 'inherits', 'deliberately', 'strict'],\n",
       "  ['An', 'mortification', 'friends', 'Certainly', 'old-fashioned'],\n",
       "  ['That', 'intelligence', 'arises', 'complacently', 'pink']],\n",
       " [['No', 'forehead', 'confesseth', 'Then', 'fold'],\n",
       "  ['this', 'course', 'dresseth', 'heavenly', 'lighten'],\n",
       "  ['the', 'fight', 'fleeth', 'once', 'drowned'],\n",
       "  ['any', 'thrice', 'winds', 'dearly', 'brotherly'],\n",
       "  ['these', 'excellent', 'horns', 'safely', 'oppress'],\n",
       "  ['These', 'dungeon', 'contendeth', 'darkness', 'fight'],\n",
       "  ['Both', 'depth', 'sorrows', 'bountifully', 'understood'],\n",
       "  ['both', 'mole', 'dwell', 'brotherly', 'unicorn'],\n",
       "  ['both', 'leisure', 'blindeth', 'pray', 'wellfavoured'],\n",
       "  ['a', 'hospitality', 'consumeth', 'constantly', 'unripe']])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_text = gutenberg.raw('austen-emma.txt')\n",
    "emma_tokens = word_tokenize(emma_text)\n",
    "\n",
    "bible_text = gutenberg.raw('bible-kjv.txt')\n",
    "bible_tokens = word_tokenize(bible_text)\n",
    " \n",
    "emma_tags = collect_words_for_tag(emma_tokens)\n",
    "bible_tags = collect_words_for_tag(bible_tokens)\n",
    "\n",
    "emma_sent = generate_sentences(sent, emma_tags, n=10, better_quality=True)\n",
    "\n",
    "\n",
    "bible_sent = generate_sentences(sent, bible_tags, n=10, better_quality=True)\n",
    "\n",
    "emma_sent, bible_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecad4e",
   "metadata": {},
   "source": [
    "### Task 3: The Viterbi algorithm\n",
    "Implement the Viterbi algorithm as introduced in the lecture (lecture 8, slide 20) and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
    "\n",
    "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found. \n",
    "\n",
    "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
    "\n",
    "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8319309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence\n",
    "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
    "\n",
    "# state transition probabilities (complete)\n",
    "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
    "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
    "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
    "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
    "\n",
    "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
    "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
    "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd60bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> tuple[List[str], float]:\n",
    "    '''\n",
    "    :param sentence: sentence that we want to tag\n",
    "    :param trans_prob: dict with state transition probabilities\n",
    "    :param emiss_prob: dict with word emission probabilities\n",
    "    :param beam: beam size for beam search. If 0, don't apply beam search\n",
    "    :returns:\n",
    "        - list with POS tags for each input word\n",
    "        - float that indicates the probability of the tag sequence\n",
    "    '''\n",
    "    states = {state for (_, state) in trans_prob.keys() if state != '<s>'}\n",
    "    n = len(sentence)\n",
    "    \n",
    "    # Initialize Viterbi matrix and backpointer matrix\n",
    "    V = [{} for _ in range(n)]\n",
    "    B = [{} for _ in range(n)]\n",
    "    \n",
    "    # Initialize start probabilities\n",
    "    for state in states:\n",
    "        V[0][state] = trans_prob[('<s>', state)] * emiss_prob.get((sentence[0], state), 0)\n",
    "        B[0][state] = None\n",
    "    \n",
    "    # Recursion step\n",
    "    for t in range(1, n):\n",
    "        for state_j in states:\n",
    "            max_prob, max_state = max(\n",
    "                (V[t-1][state_i] * trans_prob[(state_i, state_j)] * emiss_prob.get((sentence[t], state_j), 0), state_i)\n",
    "                for state_i in states if V[t-1][state_i] > 0\n",
    "            )\n",
    "            V[t][state_j] = max_prob\n",
    "            B[t][state_j] = max_state\n",
    "        \n",
    "        if beam > 0:\n",
    "            # Apply beam search pruning\n",
    "            sorted_states = sorted(states, key=lambda state: V[t][state], reverse=True)\n",
    "            top_states = sorted_states[:beam]\n",
    "            for state in states:\n",
    "                if state not in top_states:\n",
    "                    V[t][state] = 0\n",
    "    \n",
    "    # Termination step\n",
    "    last_timestep = V[-1]\n",
    "    max_final_state = max(last_timestep, key=last_timestep.get)\n",
    "    max_final_prob = last_timestep[max_final_state]\n",
    "    \n",
    "    # Path backtracking\n",
    "    best_path = [max_final_state]\n",
    "    for t in range(n-1, 0, -1):\n",
    "        best_path.insert(0, B[t][best_path[0]])\n",
    "    \n",
    "    return best_path,max_final_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a497e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['DT', 'N', 'V', 'DT', 'N'], 9.720000000000002e-06)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Viterbi(sentence, state_trans_prob, word_emission_prob, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
