{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: ML Basics - Naive Bayes Classification \n",
    "In this task, we want to build a Naive Bayes classifier with add-1 smoothing for text classification (pseudocode given below), e.g., to assign a category to a document. Use the class-skeleton provided below for your implementation.\n",
    "\n",
    "#### Naive Bayes Pseudocode\n",
    "##### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
    "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
    "$N \\leftarrow countDocs(\\mathbb D)$    \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
    "return $V,prior,condprob$\n",
    "\n",
    "##### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
    "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
    "return $argmax_{c \\in \\mathbb C} score[c]$\n",
    "\n",
    "__a) Tokenization__  \n",
    "Implement the function `tokenize` to transform a text document to a list of tokens with the regex pattern `\\b\\w\\w+\\b`. Transform all tokens to lowercase.\n",
    "\n",
    "__b) Naive Bayes \"Training\"__  \n",
    "Implement the `__init__` function to set up the Naive Bayes Model. Cf. TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$) in the pseudocode above. Contrary to the pseudocode, the `__init__` function should not return anything, but the vocabulary, priors and conditionals should be stored in class variables. We only want to keep tokens with a frequeny >= `min_count` in the vocabulary.\n",
    "\n",
    "__c) Naive Bayes Classification__  \n",
    "Implement the `classify` function to return the most probable class for the provided document according to your Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    '''Naive Bayes for text classification.'''\n",
    "    def __init__(self, docs: List[str], labels: List[int], min_count: int=1):\n",
    "        '''\n",
    "        :param docs: list of documents from which to build the model (corpus)\n",
    "        :param labels: list of classes assigned to the list of documents (labels[i] is the class for docs[i])\n",
    "        :param min_count: minimum frequency of token in vocabulary (tokens that occur less times are discarded)\n",
    "        '''\n",
    "        # your code for Task 1b) here\n",
    "        self.vocab = set()\n",
    "        self.priors = {}\n",
    "        self.cond_probs = {}\n",
    "        self.class_word_counts = defaultdict(Counter)\n",
    "        self.class_doc_counts = Counter(labels)\n",
    "        self.total_docs = len(docs)\n",
    "        \n",
    "        tokenized_docs = [self.tokenize(doc) for doc in docs]\n",
    "        \n",
    "        for label in self.class_doc_counts:\n",
    "            self.priors[label] = self.class_doc_counts[label] / self.total_docs\n",
    "        \n",
    "        for tokens, label in zip(tokenized_docs, labels):\n",
    "            for token in tokens:\n",
    "                self.class_word_counts[label][token] += 1\n",
    "\n",
    "        total_word_counts = Counter()\n",
    "        for class_counter in self.class_word_counts.values():\n",
    "            total_word_counts.update(class_counter)\n",
    "        self.vocab = {word for word, count in total_word_counts.items() if count >= min_count}\n",
    "\n",
    "        self.cond_probs = defaultdict(dict)\n",
    "        for label in self.class_word_counts:\n",
    "            total_words = sum(self.class_word_counts[label].values())\n",
    "            for word in self.vocab:\n",
    "                word_count = self.class_word_counts[label][word]\n",
    "                self.cond_probs[label][word] = (word_count + 1) / (total_words + len(self.vocab))\n",
    "\n",
    "        # print(\"Conditional Propabilities : \",self.cond_probs)\n",
    "        # print(\"class_word_counts\", class_word_counts)\n",
    "\n",
    "    def tokenize(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to tokenize\n",
    "        :return: document as a list of tokens\n",
    "        '''\n",
    "        # your code for Task 1a) here\n",
    "        tokens = re.findall(r'\\b\\w\\w+\\b', doc.lower())\n",
    "        return tokens\n",
    "\n",
    "    def classify(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to classify\n",
    "        :return: most probable class\n",
    "        '''\n",
    "        # your code for Task 1c) here\n",
    "        tokens = self.tokenize(doc)\n",
    "        class_scores = {}\n",
    "\n",
    "        for label in self.priors:\n",
    "            log_prob = log(self.priors[label])\n",
    "            for token in tokens:\n",
    "                if token in self.vocab:\n",
    "                    log_prob += log(self.cond_probs[label].get(token, 1 / (sum(self.class_word_counts[label].values()) + len(self.vocab))))\n",
    "            class_scores[label] = log_prob\n",
    "\n",
    "        return max(class_scores, key=class_scores.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) Evaluation__\n",
    "Test your implementation on the 20newsgroups dataset. If implemented correctly, with `min_count=1` your Naive Bayes classifier should obtain the same accuracy as the implementation by scikit-learn (see below for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a subset of the data for evaluvation since the time taken for entire dataset is high.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "min_count = 1\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "nb_classifier = NaiveBayesClassifier(train.data[:500], train.target[:500], 1)\n",
    "pred = []\n",
    "for i in test.data[:100]:\n",
    "    # print(nb_classifier.classify(i))\n",
    "    pred.append(nb_classifier.classify(i))\n",
    "accuracy_score(test.target[:100],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of the SKlearn Naive Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# see https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html for details\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "# print(train.target[:10])\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(train.data[:500])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x,train.target[:500])\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(test.data[:100]))\n",
    "\n",
    "accuracy_score(test.target[:100],pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Term Frequency - Inverse Document Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to use the term frequency - inverse document frequency (tf-idf) weighting method to compare documents with each other and to queries. In the end, we will apply our method to a subset of wikipedia pages (more specifically: only the introduction sections) that are linked to from the English Wikipedia page of Mannheim.\n",
    "\n",
    "In case you need to tokenize any sentences in the following tasks, please use a tokenizer from NLTK and not the ``string.split`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ To test your implementation throughout this task, you are given the example from exercise 8. Start by implementing a function ``process_docs`` that takes the provided dictionary of documents and returns the following data structures. \n",
    "\n",
    "- ``word2index``: a dictionary that maps each word that appears in any document to a unique integer identifier starting at 0 \n",
    "- ``doc2index``: a dictionary that maps each document name (here given as the dictionary keys) to a unique integer identifier starting at 0\n",
    "- ``index2doc``: a dictionary that maps each document identifier to the corresponding document name (reverse to ``doc2index``)\n",
    "- ``doc_word_vectors``: a dictionary that maps each document name to a list of word ids that indicate which words appeared in the document in their order of appearance. Words that appear multiple times must also be included multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from exercise 8\n",
    "d1 = \"cold beer beach\"\n",
    "d2 = \"ice cream beer beer\"\n",
    "d3 = \"beach cold ice cream\"\n",
    "d4 = \"cold beer frozen yogurt frozen beer\"\n",
    "d5 = \"frozen ice ice beer ice cream\"\n",
    "d6 = \"yogurt ice cream ice cream\"\n",
    "\n",
    "docs = {\"d1\": d1, \"d2\": d2, \"d3\": d3, \"d4\": d4, \"d5\": d5, \"d6\": d6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def process_docs(docs: Dict[str, str]) -> tuple[Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    :params docs: dict that maps each document name to the document content\n",
    "    :returns:\n",
    "        - word2index: dict that maps each word to a unique id\n",
    "        - doc2index: dict that maps each document name to a unique id\n",
    "        - index2doc: dict that maps ids to their associated document name\n",
    "        - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
    "    \"\"\"\n",
    "    word2index = {}\n",
    "    doc2index = {}\n",
    "    index2doc = {}\n",
    "    doc_word_vectors = {}\n",
    "\n",
    "    for i,d in enumerate(docs.keys()):\n",
    "        doc2index[d]=i\n",
    "        index2doc[i]=d\n",
    "\n",
    "\n",
    "    word_ind = 0\n",
    "    for doc,text in docs.items():\n",
    "        temp = word_tokenize(text.lower())\n",
    "        words_in_doc = []\n",
    "        for t in temp:\n",
    "            if t not in word2index:\n",
    "                word2index[t]=word_ind\n",
    "                word_ind+=1\n",
    "            words_in_doc.append(word2index[t])\n",
    "        doc_word_vectors[doc]=words_in_doc\n",
    "    \n",
    "    return word2index,doc2index,index2doc,doc_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
      "{'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
      "{0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
      "{'d1': [0, 1, 2], 'd2': [3, 4, 1, 1], 'd3': [2, 0, 3, 4], 'd4': [0, 1, 5, 6, 5, 1], 'd5': [5, 3, 3, 1, 3, 4], 'd6': [6, 3, 4, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "# The output for the provided example could look like this:\n",
    "\n",
    "# word2index:\n",
    "# {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
    "\n",
    "# doc2index:\n",
    "# {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
    "\n",
    "# index2doc\n",
    "# {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
    "\n",
    "# doc_word_vectors:\n",
    "# {'d1': [0, 1, 2],\n",
    "#  'd2': [3, 4, 1, 1],\n",
    "#  'd3': [2, 0, 3, 4],\n",
    "#  'd4': [0, 1, 5, 6, 5, 1],\n",
    "#  'd5': [5, 3, 3, 1, 3, 4],\n",
    "#  'd6': [6, 3, 4, 3, 4]}\n",
    "\n",
    "#Priniting the output of process_doc to check if the desired output is got.\n",
    "data = process_docs(docs)\n",
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Set up a term-document matrix where each column corresponds to a document and each row corresponds to a word that was observed in any of the documents. The row/column indices should correspond to the word/document ids that are set in the input dicts ``word2index`` and ``doc2index``. Count how often each word appears in each document and fill the term document matrix. \n",
    "\n",
    "_Example: The word \"beer\" with the word id_ ``1`` _appears two times in the document \"d4\" that has the document id_ ``3``. _Therefore the the entry at position_ ``[1, 3]`` _in the term-document matrix is_ ``2``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def term_document_matrix(doc_word_v: Dict[str, List[int]], doc2index: Dict[str, int], word2index: Dict[str, int]) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param doc_word_v: dict that maps each document to the list of word ids that appear in it\n",
    "    :param doc2index: dict that maps each document name to a unique id\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :return: term-document matrix (each word is a row, each document is a column) that indicates the count of each word in each document\n",
    "    \"\"\"\n",
    "    tdm = np.zeros((len(word2index), len(doc2index)))\n",
    "    for doc_name, word_ids in doc_word_v.items():\n",
    "        doc_id = doc2index[doc_name]\n",
    "        for word_id in word_ids:\n",
    "            tdm[word_id, doc_id] += 1\n",
    "    return tdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document Matrix:\n",
      "[[1. 0. 1. 1. 0. 0.]\n",
      " [1. 2. 0. 2. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 3. 2.]\n",
      " [0. 1. 1. 0. 1. 2.]\n",
      " [0. 0. 0. 2. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the term_document_matrix function\n",
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "tdm = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "\n",
    "print(\"Term-Document Matrix:\")\n",
    "print(tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ Implement the function ``to_tf_idf_matrix`` that takes a term-document matrix and returns the corresponding term frequency (tf) matrix. If the parameter ``idf`` is set to ``True``, the tf-matrix should further be transformed to a tf-idf matrix (i.e. every entry corresponds to the tf-idf value of the associated word and document). Your implementation should leave the input term-document matrix unchanged. \n",
    "\n",
    "Recall the following formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "  tf_{t,d} =\n",
    "    \\begin{cases}\n",
    "      1+log_{10}\\text{count}(t,d) & \\text{if count}(t, d) > 0\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}  \n",
    "\n",
    "$$idf_t = log_{10}(\\frac{N}{df_i})$$  \n",
    "\n",
    "$$tf-idf_{t,d} = tf_{t,d} \\cdot idf_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_idf_matrix(td_matrix: NDArray[NDArray[float]], idf: bool=True) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param td_matrix: term-document matrix\n",
    "    :param idf: computes the tf-idf matrix if True, otherwise computes only the tf matrix\n",
    "    :return: matrix with tf(-idf) values for each word-document pair\n",
    "    \"\"\"\n",
    "    num_words, num_docs = td_matrix.shape\n",
    "    \n",
    "    tf_matrix = np.zeros(td_matrix.shape)\n",
    "    \n",
    "    for doc in range(num_docs):\n",
    "        for word in range(num_words):\n",
    "            t = td_matrix[word][doc]\n",
    "            if t > 0:\n",
    "                tf_matrix[word, doc] = 1 + np.log10(t)\n",
    "                \n",
    "\n",
    "    if not idf:\n",
    "        return tf_matrix\n",
    "\n",
    "    doc_freq = np.count_nonzero(td_matrix, axis=1)\n",
    "    idf_matrix = np.log10((num_docs  / doc_freq)) \n",
    "\n",
    "    # print(tf_matrix)\n",
    "    # print(idf_matrix[:, None] )\n",
    "\n",
    "    tf_idf_matrix = tf_matrix * idf_matrix[:, None] \n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "#Testing the to_tf_idf_matrix Function\n",
    "# word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "# tdm = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "# # tfm = to_tf_idf_matrix(tdm,False)\n",
    "# # print(tdm)\n",
    "# # print(\"Term-Freq Matrix:\")\n",
    "# # print(tfm)\n",
    "# print(\"TFIDF Matrix:\")\n",
    "# print(to_tf_idf_matrix(tdm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ We want to test our implementation on our running example. First, print the tf-idf for each word of the query ``ice beer`` with respect to each document. Second, find the two most similar documents from ``d1, d2, d3`` according to cosine similarity and print all similarity values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF values for each word of the query 'ice beer' with respect to each document:\n",
      "Word: 'ice' (ID: 3)\n",
      "Document: d1, TF-IDF: 0.0000\n",
      "Document: d2, TF-IDF: 0.1761\n",
      "Document: d3, TF-IDF: 0.1761\n",
      "Document: d4, TF-IDF: 0.0000\n",
      "Document: d5, TF-IDF: 0.2601\n",
      "Document: d6, TF-IDF: 0.2291\n",
      "\n",
      "Word: 'beer' (ID: 1)\n",
      "Document: d1, TF-IDF: 0.1761\n",
      "Document: d2, TF-IDF: 0.2291\n",
      "Document: d3, TF-IDF: 0.0000\n",
      "Document: d4, TF-IDF: 0.2291\n",
      "Document: d5, TF-IDF: 0.1761\n",
      "Document: d6, TF-IDF: 0.0000\n",
      "\n",
      "Document d1: 0.0000\n",
      "Document d2: 0.0000\n",
      "Document d3: 0.0000\n",
      "Document d4: 0.0000\n",
      "Document d5: 0.0000\n",
      "Document d6: 0.0000\n",
      "\n",
      "Cosine similarity between documents d1, d2, and d3:\n",
      "Similarity between d1 and d2: 0.2017\n",
      "Similarity between d1 and d3: 0.8733\n",
      "Similarity between d2 and d3: 0.2972\n",
      "\n",
      "The two most similar documents are: d1 and d3 with a similarity of 0.8733\n"
     ]
    }
   ],
   "source": [
    "#using the cosine_similarity in sklearn to check the similarity of the vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity(query_vector: NDArray[float], doc_vectors: NDArray[NDArray[float]]) -> List[float]:\n",
    "    similarities = cosine_similarity(query_vector.reshape(1, -1), doc_vectors.T).flatten()\n",
    "    return similarities\n",
    "\n",
    "tf_idf_matrix = to_tf_idf_matrix(tdm)\n",
    "print(\"TF-IDF values for each word of the query 'ice beer' with respect to each document:\")\n",
    "query = \"ice beer\"\n",
    "\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "query_vector = np.zeros(len(word2index))\n",
    "\n",
    "for word in query_tokens:\n",
    "    if word in word2index:\n",
    "        word_id = word2index[word]\n",
    "        print(f\"Word: '{word}' (ID: {word_id})\")\n",
    "        for doc_id, doc_name in index2doc.items():\n",
    "            print(f\"Document: {doc_name}, TF-IDF: {tf_idf_matrix[word_id, doc_id]:.4f}\")\n",
    "        print()\n",
    "\n",
    "similarity_with_query = compute_cosine_similarity(query_vector, tf_idf_matrix)\n",
    "# print(\"Cosine similarity between the query 'ice beer' and each document:\")\n",
    "\n",
    "for doc_id, sim in enumerate(similarity_with_query):\n",
    "    print(f\"Document {index2doc[doc_id]}: {sim:.4f}\")\n",
    "\n",
    "doc_ids_to_compare = [doc2index[\"d1\"], doc2index[\"d2\"], doc2index[\"d3\"]]\n",
    "doc_vectors_to_compare = tf_idf_matrix[:, doc_ids_to_compare].T\n",
    "\n",
    "# print(tf_idf_matrix[:, doc_ids_to_compare])\n",
    "# print(tf_idf_matrix[:, doc_ids_to_compare].T)\n",
    "\n",
    "similarity_matrix = cosine_similarity(doc_vectors_to_compare)\n",
    "print(\"\\nCosine similarity between documents d1, d2, and d3:\")\n",
    "for i, doc_id1 in enumerate(doc_ids_to_compare):\n",
    "    for j, doc_id2 in enumerate(doc_ids_to_compare):\n",
    "        if i < j:\n",
    "            print(f\"Similarity between {index2doc[doc_id1]} and {index2doc[doc_id2]}: {similarity_matrix[i, j]:.4f}\")\n",
    "\n",
    "# Find the two most similar documents\n",
    "max_sim = -1\n",
    "most_similar_docs = (None, None)\n",
    "for i in range(len(doc_ids_to_compare)):\n",
    "    for j in range(i + 1, len(doc_ids_to_compare)):\n",
    "        if similarity_matrix[i, j] > max_sim:\n",
    "            max_sim = similarity_matrix[i, j]\n",
    "            most_similar_docs = (index2doc[doc_ids_to_compare[i]], index2doc[doc_ids_to_compare[j]])\n",
    "\n",
    "print(f\"\\nThe two most similar documents are: {most_similar_docs[0]} and {most_similar_docs[1]} with a similarity of {max_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ In a second step we want to find the documents that are most similar to a provided query. Therefore, implement the function ``process_query`` that creates a vector represention of the query. \n",
    "\n",
    "Create a vector that has an entry for each vocabulary word (words that appeared in any document), where the entry at position ``i`` indicates how often the word with id ``i`` (as indicated by ``word2index``) appears in the query. \n",
    "\n",
    "If ``tf`` is set to ``True``, you should transform all entries to tf-values. Similar, if ``idf`` is set to ``True``, return a vector with tf-idf values (cf. task __c)__). The computation of the idf values is based on the corresponding input term-document matrix.\n",
    "\n",
    "In case the query contains words that are not in any of the documents, print an appropriate error message and stop the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector (TF-IDF):\n",
      "[0.         0.17609126 0.         0.17609126 0.         0.\n",
      " 0.        ]\n",
      "Cosine similarity between the query and each document:\n",
      "Document d1: 0.2107\n",
      "Document d2: 0.8467\n",
      "Document d3: 0.2019\n",
      "Document d4: 0.1863\n",
      "Document d5: 0.5160\n",
      "Document d6: 0.2809\n"
     ]
    }
   ],
   "source": [
    "def process_query(query: List[str], word2index: Dict[str, int], td_matrix: NDArray[NDArray[float]], tf: bool=True, idf: bool=True) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    :param query: list of query tokens\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :param td_matrix: term-document matrix\n",
    "    :param tf: computes the tf vector of the query if True\n",
    "    :param idf: computes the tf-idf vector of the query if True, ignored if tf=False\n",
    "    :return: vector representation of the input query (using tf(-idf))\n",
    "    \"\"\"\n",
    "    num_words = len(word2index)\n",
    "    \n",
    "    query_vector = np.zeros(num_words, dtype=float)\n",
    "    \n",
    "    for token in query:\n",
    "        if token in word2index:\n",
    "            word_id = word2index[token]\n",
    "            query_vector[word_id] += 1\n",
    "        else:\n",
    "            print(f\"Error: Word '{token}' not found in the vocabulary.\")\n",
    "            return None\n",
    "    \n",
    "    if tf:\n",
    "        for i in range(num_words):\n",
    "            if query_vector[i] > 0:\n",
    "                query_vector[i] = 1 + np.log10(query_vector[i])\n",
    "    \n",
    "    if idf:\n",
    "        doc_freq = np.count_nonzero(td_matrix, axis=1)\n",
    "        idf_values = np.log10((td_matrix.shape[1] / doc_freq ))\n",
    "        query_vector = query_vector * idf_values\n",
    "        \n",
    "    return query_vector\n",
    "query = [\"ice\", \"beer\"]\n",
    "query_vector = process_query(query, word2index, tdm)\n",
    "\n",
    "if query_vector is not None:\n",
    "    print(\"Query vector (TF-IDF):\")\n",
    "    print(query_vector)\n",
    "    similarity_with_query = compute_cosine_similarity(query_vector, tf_idf_matrix)\n",
    "    print(\"Cosine similarity between the query and each document:\")\n",
    "    for doc_id, sim in enumerate(similarity_with_query):\n",
    "        print(f\"Document {index2doc[doc_id]}: {sim:.4f}\")\n",
    "else:\n",
    "    print(\"Query processing failed due to unknown words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f)__ Implement a function ``most_similar_docs`` that gets the vector representation of a query (in terms of counts, tf values or tf-idf values) and a term-document matrix that can either contain counts, tf-values or tf-idf values.  Compute the cosine similarity between the query and all documents and return the document names and the cosine similarity values of the top-``k`` documents that are most similar to the query. The value of ``k`` should be specified by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k most similar documents:\n",
      "Document: d2, Similarity: 0.8467\n",
      "Document: d5, Similarity: 0.5160\n"
     ]
    }
   ],
   "source": [
    "def most_similar_docs(query_v: NDArray[float], td_matrix: NDArray[NDArray[float]], index2doc: Dict[int, str], k: int) -> tuple[List[str], List[float]]:\n",
    "    \"\"\"\n",
    "    :param query_v: vector representation of the input query\n",
    "    :param td_matrix: term-document matrix, possibly with tf-(idf) values\n",
    "    :param index2doc: dict that maps each document id to its name\n",
    "    :k: number of documents to return\n",
    "    :returns:\n",
    "        - list with names of the top-k most similar documents to the query, ordered by descending similarity\n",
    "        - list with cosine similarities of the top-k most similar docs, ordered by descending similarity\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    similarity_with_query = compute_cosine_similarity(query_vector, td_matrix)\n",
    "\n",
    "    top_k_indices = np.argsort(similarity_with_query)[::-1][:k]\n",
    "    top_k_docs = [index2doc[i] for i in top_k_indices]\n",
    "    top_k_similarities = [similarity_with_query[i] for i in top_k_indices]\n",
    "    \n",
    "    return top_k_docs, top_k_similarities\n",
    "\n",
    "query = [\"ice\", \"beer\"]\n",
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "tdm = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "tf_idf_matrix = to_tf_idf_matrix(tdm)\n",
    "\n",
    "query_vector = process_query(query, word2index, tdm)\n",
    "\n",
    "if query_vector is not None:\n",
    "    top_k_docs, top_k_similarities = most_similar_docs(query_vector, tf_idf_matrix, index2doc, k=2)\n",
    "    \n",
    "    print(\"Top-k most similar documents:\")\n",
    "    for doc, sim in zip(top_k_docs, top_k_similarities):\n",
    "        print(f\"Document: {doc}, Similarity: {sim:.4f}\")\n",
    "else:\n",
    "    print(\"Query processing failed due to unknown words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Text Classification \n",
    "In this task, we want to build a logistic regression classifier to classify 20newsgroups posts. As feature representation, we want to use tf-idf vectors as just implemented.\n",
    "\n",
    "### Logistic Regression\n",
    "Implement a logistic regression classifier, similar to exercise 7. Again, you don't need to add a bias weight/feature.\n",
    "\n",
    "__a)__ Implement the `predict_proba` function in the `LogisticRegression` class below. Your function should return the output of a logistic regression classifier according to the current assignments of weights $\\mathbf{w}$, i.e., \n",
    "$$\n",
    "expit(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "You can assume that model weights are stored in a variable `self.w`. \n",
    "\n",
    "__b)__ Implement the `predict` function in the `LogisticRegression` class below. The prediction should return class `1` if the classifier output is above 0.5, otherwise `0` \n",
    "\n",
    "__c)__ Implement the `fit` function to learn the model parameters `w` with stochastic gradient descent for one epoch, i.e., going over the training data once. Store the learned parameters in a variable `self.w`. Only initialize the parameters randomly in the first training iteration and continue with learned parameters in later iterations. Make sure, that you iterate over instances in each epoch randomly.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addeded epochs to the function parameter since its asked in question but was not present in the function name. The same has been used in exercise 7 \n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "class LogisticRegression():\n",
    "    '''Logistic Regression Classifier.'''\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, x: NDArray[NDArray[float]], y: NDArray[int], eta: float=0.1, epochs: int=10):\n",
    "        '''\n",
    "        :param x: 2D numpy array where each row is an instance\n",
    "        :param y: 1D numpy array with target classes for instances in x\n",
    "        :param eta: learning rate, default is 0.1\n",
    "        :param epochs: fixed number of epochs as stopping criterion, default is 10\n",
    "        '''\n",
    "        # c)\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(x.shape[1])\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            indices = np.arange(x.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in indices:\n",
    "                xi = x[i]\n",
    "                yi = y[i]\n",
    "\n",
    "                proba = self.predict_proba(xi)\n",
    "                \n",
    "                self.w += eta * (yi - proba) * xi\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # a)\n",
    "        return expit(np.dot(self.w, x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # b)\n",
    "        return np.where(self.predict_proba(x) > 0.5,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ Apply your model to the two categories 'comp.windows.x' and 'rec.motorcycles' from the 20newsgroups data. To this end, first transform the training data to tf-idf representation with the functions `process_docs`, `term_document_matrix` and `to_tfidf_matrix`. Transform the test documents with `process_query`. Fit your model on the training data for 10 epochs. Calculate the accuracy on the test data. \n",
    "\n",
    "**Shortcut**: use the `TfidfVectorizer` from scikit learn (you may need to transform its output to a dense (array) representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9066834804539723\n"
     ]
    }
   ],
   "source": [
    "# used the shortcutTfidfVectorizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train = fetch_20newsgroups(subset='train', categories=['comp.windows.x','rec.motorcycles'])\n",
    "test = fetch_20newsgroups(subset='test', categories=['comp.windows.x','rec.motorcycles'])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_tfidf = vectorizer.fit_transform(train.data).toarray()\n",
    "x_test_tfidf = vectorizer.transform(test.data).toarray()\n",
    "\n",
    "y_train = np.array([1 if label == train.target_names.index('rec.motorcycles') else 0 for label in train.target])\n",
    "y_test = np.array([1 if label == test.target_names.index('rec.motorcycles') else 0 for label in test.target])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_tfidf, y_train, eta=0.1, epochs=10)\n",
    "\n",
    "predictions = np.array([model.predict(x) for x in x_test_tfidf])\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
